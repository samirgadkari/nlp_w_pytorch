---
title: "Ch_1_Introduction"
author: "Samir Gadkari"
date: "3/10/2021"
output: html_document
---

## Working with both Python and R in Rmarkdown using Reticulate

_This is not a part of the book - I just had to figure it out
because it was bugging me for some time. It should be in another
place, but it is so short, that I left it here._

When starting this project, I had to:

  * Run "virtualenv .venv" in the project directory
  * Run "source .venv/bin/activate" in the project directory
  * Run "pip install numpy pandas matplotlib" in the virtualenv directory
  * "install.packages("reticulate")" in the R console
  * Create a .Rprofile file in the project directory containing:
    * Sys.setenv(RETICULATE_PYTHON = ".venv/bin/python")
  * Restart R session so the new environment variable is available
  * Type reticulate::py_config() in R console to see if it has been
    correctly setup. If you are asked to install miniconda, just say no.
    
To access variables from python in R: py$var_name
To access variables from R in python: r.var_name

```{r}
library(reticulate)
```

```{python}
import torch
import numpy as np
from random import random
```

```{python}
n = [random() for i in range(300)]
```

```{r}
hist(py$n)
```

## Creating Tensors

```{python}
def describe(x):
  print("Type: {}".format(x.type()))
  print("Shape: {}".format(x.shape))
  print("Values: \n{}".format(x))
  
describe(torch.Tensor(2, 3)) # Returns a pointer to a block of memory of size
                             # 2 x 3. The values in that memory block are
                             # whatever were there before. No initialization
                             # takes place here.
```
```{python}
describe(torch.rand(2, 3))   # uniform distribution between 0 and 1
describe(torch.randn(2, 3))  # normal distribution with mean 0, sd 1
```

Any pytorch method ending in _ is an in-place operation.
It also returns a pointer to the updated data.
```{python}
describe(torch.zeros(2, 3)) # 2 x 3 tensor filled with zeros
x = torch.ones(2, 3)
describe(x)
y = x.fill_(5) # any pytorch method ending in _ is an in-place operation.
               # It also returns a pointer to the updated data
describe(x)
print("y = ", y)
```

An array or list can also be input to the Tensor() function to create
the tensor.

```{python}
x = torch.Tensor([[1., 2., 3.],
                  [4., 5., 6.]])
describe(x)
npy = np.random.rand(2, 3)
describe(torch.from_numpy(npy)) # notice dtype here is float64, since
                                # the numpy default output is float64
```

|function|return type|
|--------|-----------|
| torch.Tensor() | Float |
| torch.FloatTensor() | Float |
| torch.randn() | Float |
| torch.LongTensor() | Long |
| x.long() | Long |
| torch.tensor([[1, 2],[3, 4]], dtype = torch.int64) | Long |
| x.float() | Float |

The shape property and the size() method of a tensor gives it's shape/size.
shape is an alias for size().

## Tensor operations

  * pytorch requirement is that indices are Long tensors

```{python}
x = torch.randn(2, 3)
describe(x)
x + x == torch.add(x, x)
```

```{python}
x = torch.arange(6)
describe(x)
x = x.view(2, 3)
describe(x)
describe(torch.sum(x, dim = 0)) # sum across 0th dimension
describe(torch.sum(x, dim = 1)) # sum across 1st dimension
describe(torch.transpose(x, 0, 1)) # transpose x given the two dimensions
```

### Indexing, slicing, and joining

```{python}
x = torch.arange(6).view(2, 3)
describe(x)
describe(x[:1, :2])
describe(x[0, 1])
```

Selects the column dimension (dim = 1). Then selects the indices 0 and 2,
so the 0th and 2nd columns:
```{python}
describe(x)
indices = torch.LongTensor([0, 2])
describe(torch.index_select(x, dim = 1, index = indices))
```

```{python}
describe(x)
indices = torch.LongTensor([0, 0])
describe(torch.index_select(x, dim = 0, index = indices))
```

```{python}
describe(x)
row_indices = torch.arange(2).long()
col_indices = torch.LongTensor([0, 1])
describe(x[row_indices, col_indices])
```

### Joining tensors using concatenation functions

```{python}
x = torch.arange(6).view(2, 3)
describe(x)
torch.cat([x, x], dim = 0)
torch.cat([x, x], dim = 1)
```

Notice, when you stack tensors, the dimension given is the dimension
that will be inserted. Default is dim = 0, which is the outer brackets
of the tensor.
dim = 1 are the rows of the tensor, so they will be stacked.
dim = 2 are the single values inside the row of the tensor, so they are stacked.
```{python}
describe(x)
describe(torch.stack([x, x]))
describe(torch.stack([x, x], dim = 1))
describe(torch.stack([x, x], dim = 2))
```
### Mathematical operations on tensors

```{python}
x1 = torch.arange(6, dtype = torch.float32).view(2, 3)
describe(x1)

x2 = torch.ones(3, 2)
x2[:, 1] += 1
describe(x2)

describe(torch.mm(x1, x2))
```

### Setting up tensors for gradient calculations

```{python}
x = torch.ones(2, 2, requires_grad = True)
describe(x)
print(x.grad is None)
```

```{python}
y = (x + 2) * (x + 5) + 3
describe(y)
print(x.grad is None)
```

```{python}
z = y.mean()
describe(z)
z.backward()
print(x.grad is None)
```
The backward pass (called using z.backward()), computes a gradient value
for a tensor that participated in the forward pass.

To access a tensor's gradient, use z.grad

```{python}
z.grad
print()
y.grad
print()
x.grad
```
The leaf tensor is the one at the top - in this case x.
The message calls it the leaf tensor, because during backward(),
it is the last tensor updated.

## CUDA tensors

CUDA tensors are only distinguishable from the normal tensors in the
way that they are allocated internally. In terms of operations on CUDA tensors,
the pytorch language is the same as that for normal tensors.

The preferred method in pytorch is to write code that is device agnostic.

```{python}
print(torch.cuda.is_available())
```
```{python}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
```

Now comes the device-agnostic way of writing pytorch code
```{python}
x = torch.rand(3, 3).to(device)
describe(x)
```

Usually, pytorch code runs on CUDA. We need to monitor the running code.
To do this the monitoring code also needs to be on the same device
as the running code. If not, we will get a runtime error.

Basically, any calculations between tensors can only work if all those
tensors are on the same device.

You can always move the code by typing x.to("cpu"), or x.to("cuda"),
and then performing the calculation.

Since it is expensive to move data back and forth, usually you do all
calculations on CUDA, and then move the result into the CPU.

If you have multiple GPUs available, you can run the code like this:
CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py
Refer to the pytorch documentation for additional help for multiple GPUs.

## Exercises

1. Create a 2-D tensor and add a dimension of size 1 inserted at dimension 0.

```{python}
x = torch.Tensor(2, 3)
describe(x)
x = x[None, :, :]
describe(x)
```

2. Remove the extra dimension that was just added to the tensor

```{python}
x = x.squeeze()
describe(x)
```

3. Create a random tensor of shape 5 x 3 in the interval [3, 7)

```{python}
x = torch.rand(5, 3) * (7 - 3) + 3
describe(x)
```

4. Create a tensor with values from the N(0, 1) distribution

```{python}
x = torch.randn(2, 3)
describe(x)
```

5. Retrieve the indices of all the nonzero elements in tensor
torch.tensor([1, 1, 1, 0, 1])
```{python}
x = torch.tensor([1, 1, 1, 0, 1])
x1 = torch.nonzero(x)
describe(x1)
```
Create a random tensor of size (3, 1), and horizontally stack 4 copies of it.

```{python}
x = torch.rand(3, 1)
x1 = torch.stack([x, x, x, x], dim = 2)
describe(x1)
```

6. Create the batch matrix-matrix product of two 3-D matrices:
a = torch.rand(3, 4, 5)
b = torch.rand(3, 5, 4)

```{python}
a = torch.rand(3, 4, 5)
b = torch.rand(3, 5, 4)
describe(torch.bmm(a, b)) # torch.mm() is only for 2-D tensors.
                          # Use torch.bmm() for >2-D tensors.
```

8. Return the batch matrix-matrix product of a 3-D matrix and a 2-D matrix
a = torch.rand(3, 4, 5)
b = torch.rand(5, 4)

```{python}
a = torch.rand(3, 4, 5)
b = torch.rand(5, 4)

# We add another dimension at dim 0.
# Then we expand with:
#   - 0th dimension size is same as a's 0th dimension size
#   - rest of the dimensions are the same as b's dimensions
x = torch.bmm(a, b.unsqueeze(0).expand(a.size(0), *b.size()))
describe(x)
```

